name: ML Pipeline

on:
  push:
    branches: [ main ]
  workflow_dispatch:

jobs:
  setup-airflow:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write

    steps:
      # Step 1: Checkout Repository
      - name: Checkout Repository
        uses: actions/checkout@v3

      # Step 2: Increase Docker Memory
      - name: Increase Docker Memory
        run: |
          sudo mkdir -p /etc/docker
          echo '{"default-runtime": "runc", "runtimes": {"runc": {"path": "runc"}}, "storage-driver": "overlay2", "storage-opts": ["overlay2.override_kernel_check=true"], "default-ulimits": {"memlock": {"Name": "memlock", "Hard": -1, "Soft": -1}}, "default-shm-size": "256m", "log-driver": "json-file", "log-opts": {"max-size": "10m", "max-file": "3"}}' | sudo tee /etc/docker/daemon.json
          sudo service docker restart

      # Step 3: Set up Docker network
      - name: Create Docker network for Airflow
        run: docker network create airflow-network

      # Step 4: Start PostgreSQL container
      - name: Start PostgreSQL
        run: |
          docker run -d \
            --name postgres \
            --network airflow-network \
            -e POSTGRES_USER=airflow \
            -e POSTGRES_PASSWORD=airflow \
            -e POSTGRES_DB=airflow \
            -e POSTGRES_SHARED_BUFFERS=128MB \
            -e POSTGRES_WORK_MEM=4MB \
            postgres:13

      # Step 5: Build Airflow Docker image
      - name: Build Airflow Docker image
        run: docker build -t airflow_custom -f docker/airflow/Dockerfile .

      # Step 6: Start Airflow
      - name: Start Airflow
        run: |
          docker run -d \
            --name airflow \
            --network airflow-network \
            -p 8080:8080 \
            -e AIRFLOW__CORE__SQL_ALCHEMY_CONN="postgresql+psycopg2://airflow:airflow@postgres/airflow" \
            -e AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="postgresql+psycopg2://airflow:airflow@postgres/airflow" \
            -e AIRFLOW__CORE__EXECUTOR=SequentialExecutor \
            -e AIRFLOW__CORE__LOAD_EXAMPLES=False \
            -e AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG=1 \
            -e AIRFLOW__WEBSERVER__SECRET_KEY="${{ secrets.AIRFLOW_SECRET_KEY }}" \
            -v airflow-logs:/opt/airflow/logs \
            -v ${{ github.workspace }}/dags:/opt/airflow/dags \
            -v ${{ github.workspace }}/plugins:/opt/airflow/plugins \
            -v ${{ github.workspace }}/data:/opt/airflow/data \
            airflow_custom

      # Step 7: Migrate Airflow DB
      - name: Migrate Airflow DB
        run: |
          echo "Migrating Airflow DB..."
          docker exec airflow bash -c "
            chmod -R 777 /opt/airflow/logs &&
            airflow db migrate"

      # Step 8: Start Airflow Components
      - name: Start Airflow Components
        run: |
          docker exec -d airflow airflow webserver
          docker exec -d airflow airflow scheduler

          echo "Waiting for services to be ready..."
          for i in {1..30}; do
            if curl -s -f http://localhost:8080/health > /dev/null; then
              echo "Airflow webserver is ready!"
              break
            fi
            echo "Waiting for Airflow webserver... Attempt $i/30"
            sleep 15
          done

          if ! curl -s -f http://localhost:8080/health > /dev/null; then
            echo "Error: Airflow webserver failed to start"
            docker logs airflow
            exit 1
          fi

      # Step 9: Create Airflow user
      - name: Create Airflow user
        run: |
          docker exec airflow airflow users create \
            --username admin \
            --password ${{ secrets.AIRFLOW_PASSWORD }} \
            --firstname Anonymous \
            --lastname Admin \
            --role Admin \
            --email admin@example.com



  trigger-dag:
    runs-on: ubuntu-latest
    needs: setup-airflow
    permissions:
      contents: read
      id-token: write

    steps:
      # Step 1: Install curl and jq
      - name: Install curl and jq
        run: sudo apt-get update && sudo apt-get install -y curl jq

      # Step 2: Generate a unique DAG Run ID
      - name: Generate Unique DAG Run ID
        id: unique_id
        run: echo "dag_run_id=stock_prediction_pipeline_$(date +'%Y%m%d%H%M%S')" >> $GITHUB_ENV

      # Step 3: Trigger Airflow DAG
      - name: Trigger Airflow DAG
        id: trigger_dag
        env:
          AIRFLOW_USERNAME: admin
          AIRFLOW_PASSWORD: ${{ secrets.AIRFLOW_PASSWORD }}
          AIRFLOW_VM_IP: 34.170.107.190:8080
          AIRFLOW_DAG_ID: stock_prediction_pipeline
        run: |
          echo "Triggering Airflow DAG: $AIRFLOW_DAG_ID with unique DAG Run ID..."
          curl -X POST "http://$AIRFLOW_VM_IP/api/v1/dags/$AIRFLOW_DAG_ID/dagRuns" \
               -H "Content-Type: application/json" \
               -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
               -d "{\"dag_run_id\": \"${{ env.dag_run_id }}\"}"

      # Step 4: Wait for DAG Completion
      - name: Wait for DAG Completion
        id: wait_for_completion
        env:
          AIRFLOW_USERNAME: admin
          AIRFLOW_PASSWORD: ${{ secrets.AIRFLOW_PASSWORD }}
          AIRFLOW_VM_IP: 34.170.107.190:8080
          AIRFLOW_DAG_ID: stock_prediction_pipeline
          DAG_RUN_ID: ${{ env.dag_run_id }}
        run: |
          echo "Polling for DAG run status..."
          while true; do
            status=$(curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
              "http://$AIRFLOW_VM_IP/api/v1/dags/$AIRFLOW_DAG_ID/dagRuns/$DAG_RUN_ID" \
              | jq -r '.state')
            echo "Current DAG run state: $status"
            if [ "$status" == "success" ]; then
              echo "DAG run completed successfully!"
              break
            elif [ "$status" == "failed" ]; then
              echo "DAG run failed!"
              exit 1
            else
              echo "DAG is still running. Waiting..."
              sleep 10
            fi
          done

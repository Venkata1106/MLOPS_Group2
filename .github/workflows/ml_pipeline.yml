name: ML Pipeline

on:
  push:
    branches: [ main ]
  workflow_dispatch:

jobs:
  setup-airflow:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write

    steps:
      # Step 1: Checkout Repository
      - name: Checkout Repository
        uses: actions/checkout@v3

      # Step 2: Set up Docker network
      - name: Create Docker network for Airflow
        run: docker network create airflow-network

      # Step 3: Prepare Logs Directory
      - name: Prepare Logs Directory
        run: |
          mkdir -p ${{ github.workspace }}/logs/scheduler
          chmod -R 777 ${{ github.workspace }}/logs
      # Step 4: Start PostgreSQL container
      - name: Start PostgreSQL
        run: |
          docker run -d \
            --name postgres \
            --network airflow-network \
            -e POSTGRES_USER=airflow \
            -e POSTGRES_PASSWORD=airflow \
            -e POSTGRES_DB=airflow \
            postgres:13
      # Step 5: Build Airflow Docker image
      - name: Build Airflow Docker image
        run: docker build -t airflow_custom -f docker/airflow/Dockerfile .

      # Step 6: Create Docker Volume for Logs
      - name: Create Docker Volume for Logs
        run: docker volume create airflow-logs

      # Step 7: Start Airflow
      - name: Start Airflow
        run: |
          docker run -d \
            --name airflow \
            --network airflow-network \
            -p 8080:8080 \
            -e AIRFLOW__CORE__SQL_ALCHEMY_CONN="postgresql+psycopg2://airflow:airflow@postgres/airflow" \
            -e AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="postgresql+psycopg2://airflow:airflow@postgres/airflow" \
            -e AIRFLOW__CORE__EXECUTOR=LocalExecutor \
            -e AIRFLOW__CORE__LOAD_EXAMPLES=False \
            -e AIRFLOW__WEBSERVER__SECRET_KEY="${{ secrets.AIRFLOW_SECRET_KEY }}" \
            -v airflow-logs:/opt/airflow/logs \
            -v ${{ github.workspace }}/dags:/opt/airflow/dags \
            -v ${{ github.workspace }}/plugins:/opt/airflow/plugins \
            -v ${{ github.workspace }}/data:/opt/airflow/data \
            airflow_custom
      # Step 8: Initialize Airflow DB
      - name: Initialize Airflow DB
        run: |
          echo "Initializing Airflow DB..."
          docker exec airflow bash -c "
            chmod -R 777 /opt/airflow/logs &&
            airflow db init"
      # Step 9: Start Airflow Components
      - name: Start Airflow Components
        run: |
          docker exec -d airflow airflow webserver
          docker exec -d airflow airflow scheduler
          echo "Waiting for services to be ready..."
          for i in {1..30}; do
            if curl -s -f http://localhost:8080/health > /dev/null; then
              echo "Airflow webserver is ready!"
              break
            fi
            echo "Waiting for Airflow webserver... Attempt $i/30"
            sleep 15
          done
          if ! curl -s -f http://localhost:8080/health > /dev/null; then
            echo "Error: Airflow webserver failed to start"
            docker logs airflow
            exit 1
          fi
      # Step 10: Create Airflow user
      - name: Create Airflow user
        run: |
          docker exec airflow airflow users create \
            --username admin \
            --password ${{ secrets.AIRFLOW_PASSWORD }} \
            --firstname Anonymous \
            --lastname Admin \
            --role Admin \
            --email admin@example.com
  trigger-dag:
    runs-on: ubuntu-latest
    needs: setup-airflow
    permissions:
      contents: read
      id-token: write

    steps:
      # Step 1: Install curl and jq
      - name: Install curl and jq
        run: sudo apt-get update && sudo apt-get install -y curl jq

      # Step 2: Generate a unique DAG Run ID
      - name: Generate Unique DAG Run ID
        id: unique_id
        run: echo "dag_run_id=stock_prediction_pipeline_$(date +'%Y%m%d%H%M%S')" >> $GITHUB_ENV

      # Step 3: Trigger Airflow DAG
      - name: Trigger Airflow DAG
        id: trigger_dag
        env:
          AIRFLOW_USERNAME: admin
          AIRFLOW_PASSWORD: ${{ secrets.AIRFLOW_PASSWORD }}
          AIRFLOW_VM_IP: 34.170.107.190:8080
          AIRFLOW_DAG_ID: stock_prediction_pipeline
        run: |
          echo "Triggering Airflow DAG: $AIRFLOW_DAG_ID with unique DAG Run ID..."
          curl -X POST "http://$AIRFLOW_VM_IP/api/v1/dags/$AIRFLOW_DAG_ID/dagRuns" \
               -H "Content-Type: application/json" \
               -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
               -d "{\"dag_run_id\": \"${{ env.dag_run_id }}\"}"
      # Step 4: Wait for DAG Completion
      - name: Wait for DAG Completion
        id: wait_for_completion
        env:
          AIRFLOW_USERNAME: admin
          AIRFLOW_PASSWORD: ${{ secrets.AIRFLOW_PASSWORD }}
          AIRFLOW_VM_IP: 34.170.107.190:8080
          AIRFLOW_DAG_ID: stock_prediction_pipeline
          DAG_RUN_ID: ${{ env.dag_run_id }}
        run: |
          echo "Polling for DAG run status..."
          while true; do
            status=$(curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
              "http://$AIRFLOW_VM_IP/api/v1/dags/$AIRFLOW_DAG_ID/dagRuns/$DAG_RUN_ID" \
              | jq -r '.state')
            echo "Current DAG run state: $status"
            if [ "$status" == "success" ]; then
              echo "DAG run completed successfully!"
              break
            elif [ "$status" == "failed" ]; then
              echo "DAG run failed!"
              exit 1
            else
              echo "DAG is still running. Waiting..."
              sleep 10
            fi
          done

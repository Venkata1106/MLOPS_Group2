name: ML Pipeline

on:
  push:
    branches: [ main ]
  workflow_dispatch:

jobs:
  setup-airflow:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write

    steps:
      # Step 1: Checkout Repository
      - name: Checkout Repository
        uses: actions/checkout@v3

      # Step 2: Set up Docker network
      - name: Create Docker network for Airflow
        run: docker network create airflow-network

      # Step 3: Start PostgreSQL container
      - name: Start PostgreSQL
        run: |
          docker run -d \
            --name postgres \
            --network airflow-network \
            -e POSTGRES_USER=airflow \
            -e POSTGRES_PASSWORD=airflow \
            -e POSTGRES_DB=airflow \
            postgres:13

      # Step 4: Build Airflow Docker image
      - name: Build Airflow Docker image
        run: docker build -t airflow_custom -f docker/airflow/Dockerfile .

      # Step 5: Start Airflow container
      - name: Start Airflow
        run: |
          docker run -d \
            --name airflow \
            --network airflow-network \
            -e AIRFLOW__CORE__SQL_ALCHEMY_CONN="postgresql+psycopg2://airflow:airflow@postgres/airflow" \
            -e AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="postgresql+psycopg2://airflow:airflow@postgres/airflow" \
            -e AIRFLOW__CORE__EXECUTOR=LocalExecutor \
            -e AIRFLOW__CORE__LOAD_EXAMPLES=False \
            -v ${{ github.workspace }}/dags:/opt/airflow/dags \
            -v ${{ github.workspace }}/logs:/opt/airflow/logs \
            -v ${{ github.workspace }}/plugins:/opt/airflow/plugins \
            -v ${{ github.workspace }}/data:/opt/airflow/data \
            airflow_custom

      # Step 6: Wait for services to be ready
      - name: Wait for Airflow to be ready
        run: |
          echo "Waiting for services to be ready..."
          sleep 30  # Adjust the sleep time if necessary

          until curl -s http://localhost:8080; do
            echo "Waiting for Airflow webserver to be available..."
            sleep 5
          done

      # Step 7: Initialize Airflow DB
      - name: Initialize Airflow DB
        run: |
          echo "Initializing Airflow DB..."
          docker exec airflow airflow db init

      # Step 8: Create Airflow user
      - name: Create Airflow user
        run: |
          echo "Creating Airflow user..."
          docker exec airflow airflow users create \
            --username admin \
            --password ${{ secrets.AIRFLOW_PASSWORD }}  # Use Secrets for sensitive data
            --firstname Anonymous \
            --lastname Admin \
            --role Admin \
            --email admin@example.com

  trigger-dag:
    runs-on: ubuntu-latest
    needs: setup-airflow
    permissions:
      contents: read
      id-token: write

    steps:
      # Step 1: Install curl and jq
      - name: Install curl and jq
        run: sudo apt-get update && sudo apt-get install -y curl jq

      # Step 2: Generate a unique DAG Run ID
      - name: Generate Unique DAG Run ID
        id: unique_id
        run: echo "dag_run_id=stock_prediction_pipeline_$(date +'%Y%m%d%H%M%S')" >> $GITHUB_ENV

      # Step 3: Trigger Airflow DAG
      - name: Trigger Airflow DAG
        id: trigger_dag
        env:
          AIRFLOW_USERNAME: admin
          AIRFLOW_PASSWORD: ${{ secrets.AIRFLOW_PASSWORD }}  # Use Secrets for sensitive data
          AIRFLOW_VM_IP: 34.170.107.190:8080
          AIRFLOW_DAG_ID: stock_prediction_pipeline
        run: |
          echo "Triggering Airflow DAG: $AIRFLOW_DAG_ID with unique DAG Run ID..."
          curl -X POST "http://$AIRFLOW_VM_IP/api/v1/dags/$AIRFLOW_DAG_ID/dagRuns" \
               -H "Content-Type: application/json" \
               -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
               -d "{\"dag_run_id\": \"${{ env.dag_run_id }}\"}"

      # Step 4: Wait for DAG Completion
      - name: Wait for DAG Completion
        id: wait_for_completion
        env:
          AIRFLOW_USERNAME: admin
          AIRFLOW_PASSWORD: ${{ secrets.AIRFLOW_PASSWORD }}  # Use Secrets for sensitive data
          AIRFLOW_VM_IP: 34.170.107.190:8080
          AIRFLOW_DAG_ID: stock_prediction_pipeline
          DAG_RUN_ID: ${{ env.dag_run_id }}
        run: |
          echo "Polling for DAG run status..."
          while true; do
            status=$(curl -s -u "$AIRFLOW_USERNAME:$AIRFLOW_PASSWORD" \
              "http://$AIRFLOW_VM_IP/api/v1/dags/$AIRFLOW_DAG_ID/dagRuns/$DAG_RUN_ID" \
              | jq -r '.state')

            echo "Current DAG run state: $status"

            if [ "$status" == "success" ]; then
              echo "DAG run completed successfully!"
              break
            elif [ "$status" == "failed" ]; then
              echo "DAG run failed!"
              exit 1
            else
              echo "DAG is still running. Waiting..."
              sleep 10
            fi
          done
